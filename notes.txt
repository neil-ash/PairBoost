Algorithm idea for male vs female multiple objective gradient boosting:
- Compute gradient (derivate using dsquareloss()) for both male dataset and female datasets independently
- Pass gradients into find_pareto_descent() to get descent direction (psuedo-residuals)
- Build tree to predict psuedo-residuals

Concerns:
- What does the LIBSVM encoding mean?
- Is the dataset the same as the UCI dataset?
    - No: many fewer features in the LIBSVM dataset
- Code takes a very long time to run (>5 minutes) on UCI preprocessed data
    - Large number of features and data points
    - LIBSVM dataset has many fewer features and data points

------------------------------------ CURRENTLY -------------------------------------------------------
#  In original XGBoost code's python package, core.py
pred = self.predict(dtrain)
grad, hess = fobj(pred, dtrain)
self.boost(dtrain, grad, hess)

------------------------------------ DESIRED ---------------------------------------------------------
# dtrain1 is training data with first set of labels (first objective)
pred1 = self.predict(dtrain1)
grad1, hess1 = fobj(pred1, dtrain1)

# dtrain2 is training data with second set of labels (second objective)
pred2 = self.predict(dtrain2)
grad2, hess2 = fobj(pred2, dtrain2)

# Apply pareto to gradients (first deriv    ative of objective function)
grad_mat = np.hstack((grad1.reshape(-1, 1), grad2.reshape(-1, 1)))
grad_pareto = find_pareto_descent(grad_mat)

# Apply pareto to hessians (second derivative of objective function)
hess_mat = np.hstack((hess2.reshape(-1, 1), hess2.reshape(-1, 1)))
hess_pareto = find_pareto_descent(hess_mat)

# What will the original dtrain look like?
# Would the original dtrain dataset include each label (in this case, label1 and label2)?
# How would this be passed into the fit() function? As a 2D y array?
self.boost(dtrain, grad_pareto, hess_pareto)

------------------------------------ NOTES -----------------------------------------------------------
- Make 2 global DMatrices, dtrain1 and dtrain2
    dtrain = xgb.DMatrix(x_train, label=y_train)
- Use the above 'DESIRED' code
- This way, won't have to worry about making sure arguments line up since the dtrain arguments in
  update(self, dtrain, iteration, fobj=None) will not be used
- To test, call fit(X, y) with bogus y (all 1s or something)

- STILL NEED TO FIX MODEL INITIALIZATION??
    - What are the model's initial predictions?
        - Does this matter?
    - Where are initial predictions set?
        - Potentially: change y argument passed into fit() method to mock up the average prediction

------------------------------------------------------------------------------------------------------
Having difficulty with PyCharm. Instead, could make changes directly to pip-installed xgboost package
using atom:

Location of pip-installed xgboost package:
    /Users/Ashtekar15/anaconda3/lib/python3.6/site-packages/xgboost

------------------------------------------------------------------------------------------------------
Instead of making changes to pip-installed xgboost, make changes inside venv
    (Use shift+command+g to search)
    /Users/Ashtekar15/anaconda3/envs/ModXGBoost/lib/python3.7/site-packages/xgboost

To run test script (in /Users/Ashtekar15/Desktop/Thesis):
    conda activate ModXGBoost
    python xgb_test.py

------------------------------------------------------------------------------------------------------

In sklearn.py, class XGBModel(XGBModelBase):
   Note
   ----
   A custom objective function can be provided for the ``objective``
   parameter. In this case, it should have the signature
   ``objective(y_true, y_pred) -> grad, hess``:

------------------------------------------------------------------------------------------------------
Model is currently working for adult dataset, to do next:
- Clean up code
    - Put most code in core.py/sklearn.py, not in the xgb_test.py script
        - Move the fobj function to one of these files (?)
            - Put fobj() in sklearn.py, as __init__() argument for
              class XGBModel(XGBModelBase)
        - May need to modify a different file

        DONE

    - Inlcude a loop to iterate over all objectives (dtrain1, dtrain2, etc.)
        - Also use a loop to create DMatrix for each set of labels (all with
          the same features)
        - Move this code into a function (method of Booster (?)) in core.py
         rather than sitting out

         DONE

- Look at demos (xgboost/demo in original repo)
    - Test on Higgs dataset
    - Confirm that labels passed into fit() do not matter
- Set up version control
    - New repo (?)
    - Folder within existing repo (?)

So far, have modified files:
    core.py
    sklearn.py

------------------------------------------------------------------------------------------------------
Model currently working on both adult and higgs datasets

To do:
- Confirm that Higgs is a regression problem
  ANSWER: Ultimately a multiclass classification problem, however, the
          included prediction script takes in continuous predictions and
          thresholds them in order to make categorical predictions.

- Try to match 3.6 score on Kaggle
    - Use train/test split, don't submit predictions (?)
    - Run with MSE objective function, 120 trees
      ANSWER: Results are close (3.41 vs 3.6 MG vs XG) but not equal. Attribute
              difference to "objective='binary:logitraw'" in original XG code

- Compare results of traditional XGBoost to MGBoost on a simple regression
  problem (ex: linear regression)
  ANSWER: Results exactly the same

- Clarify questions to ask

------------------------------------------------------------------------------------------------------
New gituhb repo:
- xgboost/
- other/
    - test_scripts/
    - test_data/
- notes.txt
- README.md

------------------------------------------------------------------------------------------------------
To do before 11-4 meeting:
- Read (skim) thru Microsoft paper
- Create updated github repo
    - Share with Mehrdad
- Make changes to __init__ of MGBoost (from xgboost source code)

------------------------------------------------------------------------------------------------------
To do 11-17:
- Write function for creating features from input data
    - Make function usable with Microsoft bing dataset
- Test performance of various parameter configs
- Email function script, jupyter notebook with analysis
    - Remind Mehrdad to approve thesis in email
    - Mention questions/ideas regarding feature generation
    - Ask about meeting this coming week
- Re-read Chris Burges lambda ranking paper
- Think about research questions
    - Why does the inclusion of the A - B feature help?
    - Adding idea of lambda to xgboost ranking:
        - By modifying xgboost algorithm -- straightforward? How to?
        - Without modifying xgboost algorithm -- how?

Microsoft learning-to-rank datasets:
https://www.microsoft.com/en-us/research/project/mslr/

Concerns:
- Features will be too big to fit in memory (8GB RAM in laptop)
- Don't know which column in sample.tsv corresponds to score
- (?)

------------------------------------------------------------------------------------------------------
To do 11-19:
- Write function
- Do experiments
- Skim thru Chris Burges paper
- Think about features/performance
- Email Mehrdad results
    - MENTION THESIS APPROVAL

------------------------------------------------------------------------------------------------------
To do 11-21:
- Look at unbiased lambdamart code
- Skim thru Chris Burges paper
- Think about features/performance
- Review your summary

------------------------------------------------------------------------------------------------------
Notes 1-8:
- Can easily load train/val/test split data from Yahoo LETOR dataset using functions in unbiased
  lambdamart

  Data explanation: https://github.com/QingyaoAi/Unbiased-Learning-to-Rank-with-Unbiased-Propensity-Estimation
  Script to load data: https://github.com/QingyaoAi/Unbiased-Learning-to-Rank-with-Unbiased-Propensity-Estimation/blob/master/Unbiased_LTR/data_utils.py

- Generated features are still much too large to fit in memory (100+ GB)
- Options:
    - Train/test split with MSFT dataset
        - Compare delta_features=True vs delta_features=False
            - Report results
    - Rewrite feature generation functions for Yahoo data
        - Make cleaner than MSFT scripts
    - Fix MGBoost to only load data once
        - Modify __init__
    - Email Mehrdad
        - Mention problem/using cluster
        - Send paper https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/suml10_submission_20.pdf
        - Ask to focus on MGBoost wrt KDD
            - Mention fixing MGBoost
        - Ask about third project
    - Review theory
        - Chris Burges LTR paper
        - MGBoost problem formulation
        - Read nature paper

------------------------------------------------------------------------------------------------------
Notes 1-9:
- First test: delta_features leads to worsened performance
- Moving forward:
    - Run more times with different random seeds
        - At least 5 (?) more times
        - RESULTS: 3/5 delta_features is worse, 2/5 better (not significant)
    - Modify XGBoost hyperparameters
        - Maybe current model is too complex
            - Fewer trees
            - Shallower max depth
    - Think of explanation
        - Curse of dimensionality
        - Model complexity
            - Overfitting with more features
        - Too little data relative to number of features
    - Try using ONLY delta_features as features

------------------------------------------------------------------------------------------------------
Notes 1-10:
- Run MSFT experiments on Yahoo dataset
- Use train/valid sets only (?)
    - No need to manually train/test split
    - Don't touch test set yet (?)
- Still need to randomly sample queries in train/test sets
    - First figure out how many max queries (memory concern)

LEFT OFF:
- Yahoo_Data_Processing.ipynb
- Still need to write build_model() functionality
- Need to load test data
    - Should be very similar to train
    - Make sure to del after loading
        - del training data when testing (?)

------------------------------------------------------------------------------------------------------
Notes 1-14:
- Save generated arrays (from ENTIRE train/val set)
- Generate features for train/val sets individually
    - Save features as .npy (?)
        - Include seeds in filenames
        - Do this for all seeds/tests up front (?)
    - OR, don't save features since quick to generate features and already saved
      entire arrays
- Build and test model
    - Pass in saved train/val features
    - Test with 5 queries (67/33 train/test split)

------------------------------------------------------------------------------------------------------
Notes 1-16:
- Send Mehrdad numpy features as well
- INCLUDE RANK METRICS SCRIPT

------------------------------------------------------------------------------------------------------
Notes/Meeting 1-17:
- DeltaMART: plan? Still potential?
    - Check results/choose to proceed after running on cluster?

    Could test lambdamart on the subset of queries to do fair comparison
    LightGBM/examples/lambdarank/

- Focus on MGBoost (?)
    - Potential synthesized dataset/dataset made from existing data
    - Ideas for contexts where MGBoost would make sense:
        - Ranking (content score, freshness score, unbiasedness score)
        - Fairness across gender (ex: adult dataset)
            - Extend MGBoost to work here?

            Only need gradients, does not matter if multiple labels for the same
            data point

            Email on Monday if not done with both

        - Predict if bank should give someone a loan (predict income, credit
          history, etc. and use predicted MGBoost score as threshold)

          Ranking with multiple objectives:
          http://www.cs.toronto.edu/~mvolkovs/www2011_lambdarank.pdf

- What is the third project (?)
- Other potential projects (?)
    - Last year: non-convex optimization, escaping saddle points, DL
      optimization landscape
    - Interested in reinforcement learning, genetic algorithms projects
- Mention staying next year, still interested in research then

------------------------------------------------------------------------------------------------------
Notes/Meeting 1-20:
- Proceed with DeltaMART? Test on cluster?

    Send LambdaMART test code to Mehrdad

- Still interested in MGBoost
- Also interested in other projects:
    - Deep learning
        - Last year: non-convex optimization, escaping saddle points, DL
          optimization landscape
    - Reinforcement learning
    - Genetic algorithms
    - Theory

    Other idea related to ranking: look at paper Mehrdad sent, implement later
    ("Classification with Pairwise Information", Pareto learning to combine)

    Potential next project: federated learning (?)

    Meet to discuss other projects later, first finish current project (either
    MGBoost, DeltaMART, or combining rankers)

------------------------------------------------------------------------------------------------------
Notes 1-29:

New Projects
- Read 'Classification with Pairwise Information'
- Read/compare with 'Toward Pareto Efficient Multiple Objective Ranking'
    - Section 'A Pareto efficient learning to combine algorithm'

DeltaMART
- Write code to test pointwise approach LTR
    - To compare against lambdamart, DeltaMART

MGBoost
- Create and test on synthesized dataset
    features | label 1 | 2 * label 1 | 3 * label 1
    - Add noise to features

------------------------------------------------------------------------------------------------------
Notes 2-1:
- Made changes (mgboost implementation) to xgboost package in anaconda:
  /Users/Ashtekar15/opt/anaconda3/lib/python3.7/site-packages/xgboost
- Original xgboost package (NOT mgboost) on desktop:
  /Users/Ashtekar15/Desktop/TEMP_MGBoost/ORIGINAL_XGBoost/xgboost
- Will need to manually move the original package into anaconda package to restore original...

------------------------------------------------------------------------------------------------------
Notes 2-3:

Plan to implement Pareto logistic regression:
- Start with ML coursera implementation
    - Plan:

        Compute gradients for female, male separately
        Average gradients for each group, separately
        Call pareto function on average gradients
        Use pareto-ed gradients to update model parameters

- First, test log reg performance on adult dataset
    - Also compare to baseline (guess majority label)
    - Use both sklearn and my implementation
- Easily load adult dataset using lightsvm format (?)

------------------------------------------------------------------------------------------------------
Notes 2-4:
- First, run LTR script and save pairwise matrix W
- Implement algorithm in Jupyter

------------------------------------------------------------------------------------------------------
Questions/meeting notes 2-10:
- Classification with pairwise information
    - Problem framing:
        - Input: pairwise comparison matrices
        - Output: classification (?) model for original dataset
        - Similar to uncoupled regression:
            - Input: unlabeled data, labels (targets)
                - Also pairwise comparison data
            - Ouput: regression model
    - Submit to KDD by Thursday??
        - Main idea of paper: algorithm in Pareto LTC draft
        - Still need empirical results
            - My job?
        - My name on paper?

- Pareto logistic regression
    - Mentioned evaluating performance differently (other than accuracy for each subgroup?)

Notes from meeting:
- Do not have time to submit to KDD, target another conference instead
- Data generation: train pairwise ranker (like lambdamart) on dataset with features and (-1, +1) binary labels
    - Ranker outputs nxn pairwise comparison matrix, used for training CWPI algo
- Play with sent code, make it run
    - Clean up code as well
- Re-read CMU paper
    - "Noise-Tolerant Interactive Learning Using Pairwise Comparisons"

Quotes/other from meeting:
- Referencing Patrick McDaniel/ML security lab:
    - "He publishes one paper with Ian Goodfellow. Then the rest? That's not research."
    - "If you want to paint your house, would you hire a painter or someone off the street?"
    - "They want to mimic the behaviour of a DNN. Guess how many times they query it to get training data? 140 times. If they could
      get a DNN to learn from 140 training examples, they could buy the entire department lunch."
Department:
- "Who in the department is doing fundamental ML research? There's me, and that's it. The rest are doing applied."
- "They publish their results on MNIST. It's way too easy."
Me:
- "If you want to go to a top 5 grad school, then you must publish beforehand."
- "So do you want to go to Penn State or somewhere better? If you want to go somewhere better, then why are you asking me?"
- "You should be less focused on publishing. This is research -- we don't know which direction it will go, what will work and what won't."

------------------------------------------------------------------------------------------------------
Notes 2-11:
- Can use tensorflow directly from jupyter notebook/anaconda
    - pip installed within anaconda
    - Note that tensoflow version is '1.13.0-rc1'
        - Downgraded in order to work with Mehdi's implementation
        - Latest version is '2.1.0'
- Mehdi's implementation runs with tf version is '1.13.0-rc1' (currently installed)
- Next steps:
    - Understand code
    - Read CMU paper
    - Clean up to work with latest tf version (?)

------------------------------------------------------------------------------------------------------
Notes 2-26:
- Re-read blog post
- Skim CMU papers
- Look at algorithm
- Push to github?
    - Wait, just say you had a busy week

Notes from meeting:
- Work more similar to Tokyo paper (SU classification)
  http://proceedings.mlr.press/v80/bao18a/bao18a.pdf
  https://github.com/levelfour/SU_Classification (implementation)
  https://ieeexplore.ieee.org/document/8683118
- NIPS in two months
- Compare accuracy of our algo to Tokyo paper
    - Same amount of training data
    - Create matrix:
        - Row > col: +1
        - Row < col: 0
        - Row == col: 0.5
- Email when experiments complete
    - Potentially: Wednesday next week 4pm

------------------------------------------------------------------------------------------------------
Notes 3-2:
- Re-read CWPI draft
- Run experiment on adult dataset with pairwise label matrix
- Look at Tokyo paper/experiments/githb

Other:
- Consider testing CWPI on a very simple dataset (exam scores)

------------------------------------------------------------------------------------------------------
Notes 3-4:
- Clean up CWPI code
- Write model code as a function

NOTE) Deleted 'generate_dataset' folder from test_data, uploaded to google drive
      Code here:
      https://github.com/acbull/Unbiased_LambdaMart/blob/master/evaluation/scripts/generate_data.py

------------------------------------------------------------------------------------------------------
Questions 3-5:
- Eq 7: is f(x) (overall model's predictions) binary (-1, +1) or a continuous value? If it is continuous,
  should it be bounded (-1, +1)?
- Step 12: should be yhat = -1 instead of yhat = 0?

------------------------------------------------------------------------------------------------------
Notes 3-25:
- Compare Mehdi's implementation to mine (memory difference?)
- Make final algorithm code reusable (make into function)
- Look up datasets in 'Classification from Pairwise Similarity and Unlabeled Data' paper
  https://arxiv.org/pdf/1802.04381.pdf

------------------------------------------------------------------------------------------------------
Notes
- UPDATE NUMPY/PANDAS BEFORE SENDING CODE TO MEHRDAD
    - Done, but don't forget to do this in the future
- Did not run:
    - waveform (3 classes of waves, not binary classification)
- Data not on libsvm:
    - banana
      https://www.openml.org/t/10091
    - magic
      https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope
    - phoneme
      https://www.openml.org/d/1489
    - spambase
      http://archive.ics.uci.edu/ml/datasets/spambase
    - waveform
      https://archive.ics.uci.edu/ml/datasets/Waveform+Database+Generator+%28Version+2%29

To include in github repo:
- experiments/
- main.py
- extra.ipynb
- README.md (need to create)
    - Include link to google drive with datasets
    - Include link to google drive with results sheet

Note) moved higgs data from /Users/Ashtekar15/Desktop/Thesis/MGBoost/other/test_data/higgs-boson/data/
                         to /Users/Ashtekar15/Desktop/Thesis/MGBoost/other/test_scripts/pairwise/LambdaBoost-master/data/higgs

------------------------------------------------------------------------------------------------------
CWPI Questions:
- CAN I USE THIS AS MY THESIS?

- NEURIPS DEADLINE NOT EXTENDED?
    - May 5th abstract
    - May 12th paper

- Implementation works -- however, when I consider "The gradient of multiplicative objective for linear model"
  (second page bottom/middle), ignoring the product over K, the gradient is different (simply f'(x)). Does this
  mean that the denominator in Eij doesn't matter (equation 7)?
    - lambdas in LambdaMART are epsilons here, derivative of cost WRT predictions, not parameters

- Using decision trees rather than linear svm works much better -- why use linear SVM?
    - Can learn nonlinearities with trees, not with linear SVM...
    - Using decision trees is fine since the gradient epsilon is agnostic of weights (gradient WRT predictions, not weights)

- Potential issue with using W with {0, 0.5, 1} entries
    - A person could easily figure out the true labels given W:
        Any row with 0: true label of -1
        Any row with 1: true label of +1
    - Algorithm recovers true labels in first step, then performs boosting
    - No case where two entries have same label but one ranked above the other
    - If using ranks instead, the model's performance is upper bounded by the quality of ranks (?)
        - "ML model only as good as data it is given"

- Could try adding noise/using ranking (from svm, etc) to create W, plot performance

- To use this algorithm in practice, do you only need to have half of the W matrix (only entries above or
  below the diagonal)?
    - Equivalently: should matrix W be symmetric (across diagonal)?

- LambdaBoost often outperforms/matches SUPERVISED LEARNING baselines

- Eq 7: is f(x) (overall model's predictions) binary (-1, +1) or a continuous value? If it is continuous,
  should it be bounded (-1, +1)?

- Step 12: should be yhat = -1 instead of yhat = 0?

- Step 13: is this adding the coefficients for each model together? Or is this adding predictions of each
  individual model together?
    - In linear case: adding coefficients is the same as adding predictions (?)
    - In nonlinear cases: no way to add coefficients together (?)

- How did you come up with the equation for computing alpha (step 12)?

- How did you come up with loss function/intution (eq 1)?
    - Cases where nonzero

------------------------------------------------------------------------------------------------------------------------
To do:
- Revisit creating W from ranking
    - Situation where two points have same label but one is ranked higher (since closer to decision boundary)
- Confirm that the denominator (eq 7) for computing epsilon doesn't matter?
- Confirm labels (step 8) are equal to original labels after/during one iteration?

------------------------------------------------------------------------------------------------------------------------
Papers read:
- Regression with comparisons: escaping the curse of dimensionality with ordinal information (CMU 2019)
    - Two regression w/ comparison algorithms:
        - R^2 Ranking-Regression
            - Isotonic regression (regression consistent with ranking)
            - Uses nearest-neighbor
            - Escapes curse of dimensionality: error rate only depends on num labels and num comparisons, not on
              dimensionality of features
        - CLR Comparison linear regression
            - Converts regression to series of classification problems, used to find weight vector
    - Experiments:
        - Synthetic data
        - Predicting age from photographs
        - Estimating AirBnB prices

- Noise-tolerant interactive learning using pairwise comparisons (CMU 2017)
    - Reduces problem of classification given pairwise information to binary search
    - Frames as active learning with labeling and comparison oracles
    - Discusses noisy data

- Blog (CMU)
    - https://blog.ml.cmu.edu/2019/03/29/building-machine-learning-models-via-comparisons/
    - Summarizes main ideas in two papers above

- Classification from pairwise similarity and unlabeled data (Tokyo 2018)
    - Learning from similar-unlabeled data
    - Creates and optimizes loss function for setting
    - Linear model
    - I did experiments with these datasets

- Classification from pairwise similarities/dissimilarities and unlabeled data via empirical risk minimization (Tokyo 2019)
    - Extension of similar-unlabeled (2018 paper) to dissimilar-unlabeled and similar-dissimilar
    - Dissimilar-unlabeled and similar-dissimilar combined is best performance
    - Related setup to Tokyo 2018 paper

- Uncoupled regression from pairwise comparison data (Tokyo 2019)
    - Uncoupled regression: given unlabeled features and labels, don't have correspondence
    - Solves using pairwise comparisons
        ex) Point Xi has larger label value than point Xj
    - Show for linear models, URPC has similar performance as supervised learning
    - Uses empirical risk minimization (rather than ranking) with two methods:
        - Risk approximation
        - Target transformation

- The power of comparisons for actively learning linear classifiers (UCSD 2019)
    - Hard time following...
        - Sphere-packing? Caps? Gamma-ball?
    - Shows active learning requires exponentially fewer samples (with assumptions) when given comparison information
    - Discusses RPU (reliable and probably useful) model
        - Cannot be wrong, can say idk
        - Shows not intractable wrt num labels

- Learning from noisy similar and dissimilar data (Tokyo 2020)
    - Similar to other Tokyo papers, with focus on noise in real-world data

- Binary classification from positive-confidence data (Tokyo 2018)
    - Shows how to learn binary classifier given only positive data along with confidence scores

- Learning to Rank: From Pairwise Approach to Listwise Approach (Microsoft 2007)
    - Overview of LTR development

------------------------------------------------------------------------------------------------------------------------
Meeting notes 4-20:
- Run experiments with number of labeled points (m) varying
    - Replace unknowns with 0.5
    - Create plots of accuracy or error as m goes from 10 (?) -> (n(n - 1) / 2)
        - Since W is symmetric, don't need n^2
    - Gives fair comparison to Tokyo paper
- Also run experiments with W created from ranking
    - From pairwise similar-dissimilar data
        - Idea: want to perform classification without any labeled data at any point
    - Look at Mehdi's code
        - Using SVM rank
- Theory needed for NeurIPS
    - Need reasoning for why we don't need log(n) labels
- NeurIPS deadline has been extended
    - May 27: abstract
    - June 3: paper
- I can use this project as my thesis

------------------------------------------------------------------------------------------------------------------------
Notes 4-21:

"Classification from Pairwise Similarity and Unlabeled Data":
- Figure 4: {200, 400, 800, 1600} data with half similar, half unlabeled
- Figure 5: 200 similar data, {200, 400, 800, 1600} unlabeled data
- Table 4: 500 similar, 500 unlabeled, 100 test

My experiments:
- Dataset sizes:
    - TRAIN_SIZE = {200, 400, 800, 1600} total train data
    - NUM_LABELS = {100, 200, 400, 800} labeled train data
    - TEST_SIZE  = {100, 200, 400, 800} test data
- Record both train and test accuracy/error
- Not varying NUM_LABELS:TRAIN_SIZE ratio (?)
    - Could also try a set TRAIN_SIZE, vary NUM_LABELS:
        - TRAIN_SIZE = 1000
        - NUM_LABELS = {100, 200, 300, 400, 500, 600, 700, 800, 900, 1000}
        - TEST_SIZE = 100

Questions/ideas:
- CMU binary search paper: pairwise comparisons needed include less information than W matrix
    - Only need one entry per column, one entry per row, not on diagonal?
    - TRACE THRU
- Complete W matrix gives the same information as true labels -- should be comparing performance against supervised
  models?
    - Also compare with limited data
    - Even in partially filled out W: still given m labels -- any better than a supervised model trained on only m
      labels?
- W's pairwise comparisons gives more information than similar, dissimilar, and similar-dissimilar
    - W tells if similar or dissimilar in addition to telling which is more likely to be +1
- RankSVM experiment: is partially filled out W the input?
- Still haven't considered case where yi = -1, yj = -1, but Wij = 1
    - Or with each flipped
    - Idea: two patients, neither have cancer, one is predicted as more likely to have cancer than the other

------------------------------------------------------------------------------------------------------------------------
TRY INCREASING T TO 25 OR 50 TO GET ALGORITHM TO STABILIZE WHEN MAKING PLOTS
    - Issue with flipping predictions when T, max_depth too low...
LOOK AT STD FOR EVERY m VALUE

------------------------------------------------------------------------------------------------------------------------
Pairwise Ranker
- Randomly sample m comparisons from above diagonal of nxn matrix
- Train LinearSVR
    - Features: {Xi, Xj}
    - Labels: {0, 0.5, 1}
        - 0   if yi < yj
        - 0.5 if yi = yj
        - 1   if yi > yj
- Fill in entire nxn matrix using predictions from LinearSVR
    - minmax_scale predictions 0 -> 1

To do Wednesday:
- Review that two new functions work properly (generate_rank_labels, generate_rank_W)
    - Comment them
- Consider using m to be num comparisons rather than m(m-1)/2 comparisons
    - Already fixed, experiment more
- Run on adult and susy datasets
    - For more iterations
- Consider running experiment on subsets of data
    ex) m = 1600 is largest, m = 800 has all samples from 1600, etc.
- Compare 500-500-100 comparisons-unlabeled-testing to accuracies in Tokyo tables
- Compare quality of generated W by ranks vs actual labels

------------------------------------------------------------------------------------------------------------------------

Pairwise comparisons:
- Hopkins et al., 2019
    - The power of comparisons for actively learning linear classifiers
    - UCSD 2019
- Bao et al., 2018
    - Classification from pairwise similarity and unlabeled data
    - Tokyo 2018
- Dan et al., 2020
    - Learning from noisy similar and dissimilar data
- Ishida et al.,2018
    - Binary classification from positive-confidence data
- Shimada et al., 2019
    - Classification  from  pairwise  similarities/dissimilarities and unlabeled data via empirical risk minimization
    - Tokyo 2019 (first)
- Xu et al., 2017, 2018
    - Noise-tolerant interactive learning using pairwise comparisons
        - CMU 2017
    - Nonparametric regression with comparisons: Escaping the curse of dimensionality with ordinal information
        - CMU 2019

Learning-to-rank:
- Cao et al. (2007)
    - Learning to rank: from pairwise approach to listwise approach
- Burges(2010)
    - From ranknet to lambdarank to lambdamart: An overview
- Lambdamart (Chapelle and Chang, 2011)
    - Yahoo! learning to rank challenge overview

Other papers (not yet cited):
- Pairwise Preference Learning and Ranking
    - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.7067&rep=rep1&type=pdf
- Active classification with comparison queries
    - https://arxiv.org/pdf/1704.03564.pdf

To do:
- Read/take notes on:
    - Learning from noisy similar and dissimilar data (Tokyo 2020)
    - Binary classification from positive-confidence data (Tokyo 2018)
    - Learning to Rank: From Pairwise Approach to Listwise Approach (Microsoft 2017)

------------------------------------------------------------------------------------------------------------------------
5-18 Meeting Prep

I can do:
- Fix algorithm psuedocode in overleaf
    - Also add to/fix algorithm explanation
    - Include derivation??
- Write broader impact section
- Experiments:
    - How accurate is the model learned with our algorithm compared with a simple binary classification when labeled data is available.
        - Done for 11 datasets in Tokyo paper
        - Also for KNN, linear SVM, random forest, xgboost
    - What is the effect of total number of data and total number of pairwise comparisons in the accuracy?
        - Done for 3 datasets in Tokyo paper (see plots)
        - QUESTION ABOUT FLIPPING
    - How our algorithm is compared with state-of-the-art algorithms in terms of accuracy?
        - Done for 3 datasets in Tokyo paper (see plots)
        - Can easily do for other datasets/data sizes for fair comparison
- Other potential experiments:
    - Adding noise
    - Evaluating performance of W created from ranking vs filled in with 0.5

Other:
- Should include more than just adult/synthetic dataset
- Flipping prediction question
- Use my implementation over Mohammad's
- Deal with Microsoft paper with similar content??
    - Should not have direct quotes from it??
- Include derivations in paper??

------------------------------------------------------------------------------------------------------------------------
5-19 Meeting with Mohammad

- Algorithm input
    - S: unlabeled
    - P: pairwise
        - What is y here?
    - sigma: confidence
        - Why is this necessary/value other than 1?
- SU_classification, learn the ranker: Why is rnd_c necessary?
    - Why subtract x_pair, y_pair?
        - Is this only using dissimilar pairs??
    - Interpretation: used to generate pairs on both sides of the diagonal of W
        - Why no need for pairs with the same labels?
- ASK: is my method better?
    - My method: given m comparisons in W matrix, use XGBRegressor to predict the other n - m entries
- In lambdaboost/SU notebooks: why measure clf performance instead of overall performance?
- Could also create nonlinear ranker?
    - Rather than linear SVM
- Could also create nonlinear submodels?
    - ex) decision trees rather than linear SVMs
- Question about predictions flipping
    - Generally: overly optimistic to take min error?
    - May not be an issue with change
- Experiments: compare to Tokyo table?
    - For many datasets
- Mehrdad: change related works
    - Do we need three citations for LTR?
    - How should I change it?

------------------------------------------------------------------------------------------------------------------------
5-20

Questions:
- Boosting often makes performance worse with limited data
    - Could stop early/choose t value with best performance?
- How to setup experiment?
    - Two sources of variability:
        - Model with fixed subset of data
        - Changing subsets of data

To do:
- Setup to include 50 pairs
    - "if table"
    - Fix table_results function
- Fix hyperparameters
    - Consistent everywhere
- Return min error of subtree
- Set random_seed for sampling of pairs

NEXT TIME:
- UPLOAD TO GITHUB
- Review code for bugs
- Run table experiments

------------------------------------------------------------------------------------------------------------------------
5-21

To do:
- Include X_train, y_train, X_test, y_test

Next time:
- Send results to Mohammad, Mehradad (?)
- Do plots (?)
    - Which datasets?

------------------------------------------------------------------------------------------------------------------------
5-22

To do:
- SEND RESULTS TO MEHRDAD
- ADD RESULTS TO PAPER
- Fix plot function
    - Include 'shadow'
- Get plots for:
    - magic (svm)
    - magic (tree)
    - banana (tree) ?
    - phishing (either) ?
- Setup experiment for adding unlabeled data

To use:
- Varying m:
    - Phishing, Tree
    - Adult, SVM

------------------------------------------------------------------------------------------------------------------------
5-24

To do (overall):
- Ask Mohammad about using different datasets in new experiments
    - Run m (labeled) experiment on different datasets
    - Put m (labeled) and n (ratio) plots in paper
- Bold table entries
- Cleanup code according to NeurIPS github guidelines
- Write broader impact

Table Columns:
Dataset | m | svm | tree | SDDU Squared | SDDU Double Hinge

Future ideas/experiments:
- Run experiments on regression datasets with a threshold
- (On paper) understand how algorithm works with pairwise comparisons one greater than other but both same label

------------------------------------------------------------------------------------------------------------------------
Old concerns:

\textcolor{red}{(Neil) Other ideas:}
\begin{itemize}
\item \textcolor{red}{(\#1 Ranking vs Regression)} Why perform steps 3 and 4 of the algorithm instead of just training a pairwise regressor to fill in entries in W directly? Could reframe the problem as given some entries in W (Wij is Probability rank i higher than j) rather than given y for pairs. This would be solving the same problem more directly (?).
\item \textcolor{red}{(\#2 Non-trivial comparisons)} In our experiments, we have a contrived set up -- we take a fully labeled dataset, then take a subset of points as pairs and assign pairwise labels based on the difference in the pair's true labels. This setup ignores a very important consideration. In the real world, we would have pairs (xi, xj) such that xi is more likely to be positive than xj, but
\textit{both xi and xj are actually negative} (or actually positive). If we ignore this consideration, the problem is trivial, since we can easily recover true labels from the matrix W (ie any row in W with an entry of 1 -> assign true label of 1, any row in W with an entry of 0 -> assign true label of -1).
    \begin{itemize}
        \item This problem arises in steps 3 and 4 of the current algorithm. How do you learn the pairwise ranker in step 3? In experiments, we learned a linear SVM classifier trained on the difference in features used to predict the difference in labels. Specifically, for all xi such that yi = 1 and for all xj such that yj = -1, (xi, xj) has label 1 and (xj, xi) has label -1. We never test that this approach actually works when you have non-trivial pairs as described above ((xi, xj) such that xi is more likely to be positive than xj, but both xi and xj have the same true label).
        \begin{itemize}
            \item Note that when we make predictions to fill in the W matrix, we expect to create pairs as described above, since we use the distance to the decision boundary as $\boldsymbol{w}^{\top}\bit{x}_i$ in step 4. This could be another potential problem/limitation: the quality of W depends on the performance of the linear SVM. We could get better performance using a nonlinear regressor (see above: \#1 Ranking vs Regression).
        \end{itemize}
        \item This problem may not seem important, since it is dealing with creating pairs before even using the boosting algorithm. However, it is important -- in fact it is the whole point of the algorithm. Specifically, the performance of the boosting algorithm depends on how well-formed the W matrix is. If the W matrix is "perfect" (Wij is 1 if yi > yj, 0.5 if yi = yj, and 0 if yi < yj) then the boosting algorithm simply recovers the true labels in the first step and learns as if it is a fully supervised problem.
    \end{itemize}
\item \textcolor{red}{(\#3 Experiments using regression datasets)} To test performance with non-trivial pairwise comparisons, we could use a regression dataset with some threshold on the labels to transform the problem into classification. For example, we could have a dataset with features as information about people and labels as people's income. We could then set the threshold at \$100k and treat it as a classification problem. This is useful because then we could sample pairs and get non-trivial comparisons (ie Bob makes more than Joe, but Bob and Joe each make less than \$100k). This gives us more information to work with vs binary classification datasets, and we would not have to rely on a linear SVM extrapolating on points within a pair. We could then compare the performance of PairBoost to supervised classification algorithms on these datasets.
\end{itemize}

------------------------------------------------------------------------------------------------------------------------
8-20

Concerns/Ideas:
- Main concern: empirical results are bad when using linear SVM pairwise ranker
    - Requires two levels of linear separability (?)
        - Pairwise ranker
        - Pairwise classifier
- My approach using XGBoost regressor did not make sense
    - No nontrivial comparisons
        - Why not just learn a supervised model?
- Proposed method of ranking with regression also does not make sense
    - Assumes full W matrix
    - Unecessary since Mohammad's way works
- Question: what are we given (ie what is yi in step 1 of algorithm)?
    - Mohammad's approach: only learn from dissimilar pairs, know one example is more likely to be positive than another
    - Possible to also include similar pairs?
        - In Mohammad's approach: these points would be right on the decision boundary
    - Probably: pair of points, which one is more likely to be positive
- "Linking": if A < B and B < C, then A < C
    - Could use to create more labels for pairs

Plan:
- First, get results with SVM pairwise ranker

------------------------------------------------------------------------------------------------------------------------
8-22

Notes:
- Phishing plots
    - nSD (plots): good decrease
    - nU (unlabeled): bad, all over the place
    - ratio: very good decrease
    - labeled: very good descrease

Mapping from old -> new terms:
    plots - SD - labeled vary, unlabeled fixed
    unlabeled - U - labeled fixed, unlabeled vary
    labeled - m - labeled vary, total n fixed
    ratio - n - total n vary, ratio m / n fixed

At end of session:
- Have plots for phishing and spambase
    - All plots look good except for U (unlabeled) experiments
- Next time, get table results for 50 trials, re-read draft on overleaf, prep for Mohammad

------------------------------------------------------------------------------------------------------------------------
8-24

Thoughts/explanation of algorithm:
- Idea: given pairwise comparisons (ie Bob is more likely to have cancer than Joe) learn classifier to predict if new
  paitients have cancer
- Algorithm explanation:
    - Input: labeled pairwise comparisons (-1 or 1 if x1 < x2 or x1 > x2,  respectively)
    - First, learn pairwise ranker on labeled pairwise comparisons
    - Next, make predictions using pairwise ranker to fill in entries in W (uses given pairwise comparisons -1 or 1 to
      fill in missing pairwise comparisons as probabilities 0 <-> 1)
    - At this stage, true labels can be recovered (or approximated) from sign(sum(row_i) - sum(col_i)) and this is now
      a typical supervised learning problem
    - However, we take it a step further, and instead create a cost function based on the magnitude of wi, where
      wi = (sum(row_i) - sum(col_i)), accomplished using sampling based on wi. This includes more information about the
      importance of each point, using the 'extra' information gathered from the input pairwise comparisons vs typical
      binary labels
    - Finally, we perform gradient boosting, with each boosting iteration as a binary classification problem

Other idea: use all pairs to learn SVM (all positive and -all negative)

------------------------------------------------------------------------------------------------------------------------
8-25

To ask/mention:
- My initial approach with XGBoost regressor, why it was wrong

- What is yi in step 1 of algorithm?
    - 0/-1 if xi1 more likely to be positive than xi2
    - 1/+1 if xi2 more likely to be positive than xi1

- Experiments: don't know which subset of data was used in the Tokyo paper
    - This is important because sampling different subsets of data is source of lots of variability
    - Also, cannot use Tokyo implementation available on github because of error/lack of scalability
    - Just compare best seed? Is this cheating? Too similar to Tokyo paper if we just copy their table?
        - Should not do since source of variability...

- Our proposed method solves a fundamentally different problem vs the Tokyo paper
    - Ours: given pairwise comparisons, ie example 1 is more likely to be positive than example 2
    - Theirs: given similar/dissimilar pairs
        - Theirs is richer in the sense that it is definite (?) (pairs are either similar or dissimilar) whereas we
          could have cases where example 1 is more likely to be positive than example 2 but both are negative
            - Note that these comparisons are what makes our approach nontrivial
        - Apples to oranges comparison of which is richer, they are just different...
    - Other papers/baselines to compare against?
        - Lower baselines: k-means, constrained k-means (?), semi-supervised spectral clustering (?), information
         theoretical metric learning (?) (like in Tokyo paper)
        - Upper baselines: fully supervised
        - Other lower baselines?
            - My experience: haven't seen any papers really like ours...

- What more is left to do?
    - Include comparisons with other baselines, fully supervised baseline in table?
    - Analysis of sample complexity?
    - Derivation of algorithm?
    - Deadline in early October

- Question about original Microsoft paper

- Working with nonlinear models instead?
    - Pairwise ranker: svm with rbf/gaussian kernel
    - Boosting models: decision tree, nonlinear svm, etc.
    - Not apples to apples comparison with Tokyo paper?
        - Tokyo model is linear

Notes from meeting:
- Tokyo 2019 SDU: may use more information than we use, recovers more information about true labels
- yi is binary
- Compare ours to Tokyo SU, try to get working again

Potential baselines papers:
- https://pdfs.semanticscholar.org/7346/a3b48d960aab83c96e877f253d5a7df4295d.pdf
- https://icml.cc/imls/conferences/2007/proceedings/papers/192.pdf

Potential baseline implementations:
- K-means: sklearn
- Constrained k-means: https://github.com/Behrouz-Babaki/COP-Kmeans
- Agglomerative clustering/Ward hierarchical clustering/Spectral clustering: sklearn
  https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering
- Information theoretical metric learning: https://pydml.readthedocs.io/en/latest/dml.html#module-dml.itml

------------------------------------------------------------------------------------------------------------------------
8-27

To do:
- Get SU results
- Message Mehrdad

Hi Mehrdad, I have been working with Mohammad on the PairBoost project. I'm working on getting updated results compared to the 2018 Tokyo similar-unlabeled classification paper.

I spoke with Mohammad to clarify the problem formulation for PairBoost. The algorithm takes as input unlabeled data and pairwise data. The pairwise data is represented as {(x1i, x2i, yi)}. Here, yi indicates which point's true label is more likely to be positive, correct? Additionally, yi should be a binary value (+1 if x1i more likey to be positive than x2i, -1 if x2i more likey to be positive than x1i) NOT a continuous value, correct?

Currently in the experiment setup, I train a linear SVM on the difference in feature values for dissimilar...........

------------------------------------------------------------------------------------------------------------------------
8-29

Ideas:
- PairBoost only recovers the true labels if we have trivial comparisons
- It is impossible to recover true labels if we have nontrivial comparisons (specifically, we have an ordering
  x1 < x2 < x3...., but it is impossible to recover the threshold between positive/negative examples)
    - This is obvious when considering the problem formulation -- all we have is an ordering
    - Reason why there are no other proposed approaches like ours...
    - See CMU blog post https://blog.ml.cmu.edu/2019/03/29/building-machine-learning-models-via-comparisons/
- If we have nontrivial comparisons, there is a potential disconnect between what the boosting optimizes for
  (consistency with the W matrix) and the goal of classification (predicting true labels)
    - Looked at examples with nontrivial comparisons, yi = sign(wi) only reveals which points are more likely to be
      ranked above in general (ie cutoff is 50% kinda)

Plan:
- Add preliminary step with SVM ranker
- Get results
    - vs previous
    - vs SU

Takeaways (rebalancing the w8a dataset):
- Rebalancing the training set improves performance, even when the testing set is not rebalanced
    - Rebalanced: closer to 50/50 positive/negative examples
    - Improvement from ~60% -> ~75%+
- Hypothesis: maximized number of different-label comparisons, making it easier for PairBoost to learn threshold

************************************************************************************************************************
-> BIG IDEA:
--> Given pairwise comparisons, only additional piece of information necessary is the proportion of true positives in
    the training data (class prior, pi+ in Tokyo papers). This allows you to recover the threshold between positives and
    negatives. You may not always know this class prior for your training data, but it may be possible to estimate it
    from the population or using background knowledge about the problem.
************************************************************************************************************************

------------------------------------------------------------------------------------------------------------------------
8-30

Question:
- Why does boosting with linear models (linear SVM) make sense? Wouldn't this be just as good as learning a single
  linear model?
    - Because logistic regression model is not linear after applying sigmoid function?
        - Not relevant in this situation, just adding linear coefficients...

------------------------------------------------------------------------------------------------------------------------
9-2

Ideas:
- Simple CWPI algorithm with class prior:
    1. Learn pairwise ranker
    2. Make predictions on pairs not given
    3. Fill in W matrix
    4. For each point, assign weight = sum(row) - sum(col)
    5. Sort weights
    6. Assign the first (class prior) proportion of the points a negative label, assign top (1 - class prior) points to
       positive label
    7. Learn supervised classifer on predicted labels
- Could also incorporate class prior into existing PairBoost algo
    - Step 9: extract labels, rather than using sign(weights)
        - Sort weights
        - Assign the first (class prior) proportion of the points a negative label, assign top (1 - class prior) points
          to positive label
        - Could also treat as regression problem and set cutoff on predictions to +1 or -1
            - Incorporates more information from the W matrix
            - Potentially worse since not optimizing for classification (?)












.
