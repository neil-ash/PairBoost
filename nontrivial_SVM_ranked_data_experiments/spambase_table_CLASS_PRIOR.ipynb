{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys; sys.path.insert(0,'..')\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/spambase/spambase.data')\n",
    "X = data.iloc[:, :-1].values\n",
    "y  = data.iloc[:, -1].values\n",
    "\n",
    "# {0, 1} -> {-1, 1}\n",
    "y = y * 2 - 1\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    X[:, i] = scaler.fit_transform(X[:, i].reshape(-1, 1)).ravel()\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, stratify=y, \n",
    "                                                    shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_arr, tree_arr = data_size_experiment(X_train, y_train, X_test, y_test,\n",
    "#                                          experiment_type='table', kernel='linear',\n",
    "#                                          random_state=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_results(svm_arr, tree_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq1 = np.sum(y == 1) / y.size\n",
    "# max(eq1, 1 - eq1)\n",
    "eq1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRELIMINARY\n",
      "Training accuracy of supervised SVM: 0.82\n",
      "\n",
      "GENERATE W\n",
      "[[-1 -1]\n",
      " [-1  1]\n",
      " [ 1 -1]\n",
      " [ 1  1]] [61 49 56 34]\n",
      "Training accuracy of pairwise rank SVM: 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)\n",
    "\n",
    "X_train, y_train, X_test, y_test = X_train[:200], y_train[:200], X_test[:500], y_test[:500]\n",
    "\n",
    "rank = generate_preliminary_pairs(X_train, y_train, kernel='linear', random_seed=36298)\n",
    "W = generate_from_preliminary_W(X_train, y_train, rank, m=200, kernel='linear', random_seed=247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_svm_lambdaboost(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     W,\n",
    "#     prior=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def new_svm_lambdaboost(X_train, y_train, X_test, y_test, W, prior, T=20, sample_prop=1, random_seed=None, verbose=True):\n",
    "# \"\"\" Generates linear svm lambdaboost classifier \"\"\"\n",
    "prior = 0.4\n",
    "T=20; sample_prop=1; random_seed=None; verbose=True\n",
    "\n",
    "def class_prior_svm_lambdaboost(X_train, y_train, X_test, y_test, W, prior, T=20, sample_prop=1, random_seed=None, verbose=True):\n",
    "    \"\"\" Generates linear svm lambdaboost classifier using class prior information \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('WITH CLASS PRIOR')\n",
    "        print('LINEAR SVM SUBMODELS')\n",
    "        print('Using %d classifiers and sample proportion of %d' % (T, sample_prop))\n",
    "        if random_seed:\n",
    "            print('Random seed %d', random_seed)\n",
    "\n",
    "    # Constants\n",
    "    m = X_train.shape[0]\n",
    "    n = X_train.shape[1]\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Initialize model parameters\n",
    "    f_intercept = 0\n",
    "    f_coefficient = np.zeros(n)\n",
    "\n",
    "    # Initialize counters\n",
    "    t = 1\n",
    "    alpha = [0.0]\n",
    "    acc_train_ls = []\n",
    "    acc_test_ls  = []\n",
    "\n",
    "    # Training\n",
    "    while t <= T and alpha[-1] >= 0:\n",
    "\n",
    "        # Step 6: compute epsilon\n",
    "        curr_pred = np.dot(X_train, f_coefficient) + f_intercept\n",
    "        # Scale predictions, works well empirically\n",
    "        curr_pred = minmax_scale(curr_pred, feature_range=(-1, 1))\n",
    "        # Remember that W is passed as a list of arrays!\n",
    "        epsilon = cal_uncertainty(curr_pred, [W])\n",
    "\n",
    "        # Step 7: compute weights\n",
    "        weight = cal_weights(epsilon)\n",
    "\n",
    "        # Step 8: extract labels using class prior\n",
    "        weight_idx = np.argsort(weight)\n",
    "        X_new = X_train[weight_idx]\n",
    "        y_new = np.concatenate((-np.ones(m - int(m * prior)), np.ones(int(m * prior))))\n",
    "\n",
    "        # Step 9: create training (sample) data by sampling based on weights\n",
    "        # Sorting weights: necessary for sampling by prob in np.random.choice\n",
    "        weight = np.sort(weight)\n",
    "        p_weight = np.abs(weight)\n",
    "        p_weight /= np.sum(p_weight)\n",
    "        sample = np.random.choice(m, size=m*sample_prop, replace=True, p=p_weight)\n",
    "        X_sample = X_new[sample]\n",
    "        y_sample = y_new[sample]\n",
    "\n",
    "\n",
    "        # Step 10: learn binary classifier on training (sample) data\n",
    "        clf = LinearSVC(max_iter=1000)\n",
    "        clf.fit(X_sample, y_sample)\n",
    "\n",
    "        # Step 11: predict labels using current classifier\n",
    "        y_pred = clf.predict(X_train)\n",
    "\n",
    "        # Step 12: compute weight of current classifier\n",
    "        alpha_t = cal_alpha(y_pred, epsilon)\n",
    "\n",
    "        # Make sure alpha is valid\n",
    "        if np.isnan(alpha_t) or np.isinf(alpha_t):\n",
    "            print('Alpha invalid, terminated')\n",
    "            break\n",
    "\n",
    "        # Step 13: update final classifier\n",
    "        f_coefficient += alpha_t * clf.coef_.ravel()\n",
    "        f_intercept += alpha_t * clf.intercept_\n",
    "\n",
    "        # Update loop\n",
    "        alpha.append(alpha_t)\n",
    "        t += 1\n",
    "\n",
    "        # Evaluation\n",
    "        y_train_pred = np.dot(X_train, f_coefficient) + f_intercept\n",
    "        y_test_pred = np.dot(X_test, f_coefficient) + f_intercept\n",
    "        y_train_pred = np.sign(y_train_pred)\n",
    "        y_test_pred = np.sign(y_test_pred)\n",
    "\n",
    "        acc_train_curr = accuracy_score(y_train, y_train_pred)\n",
    "        acc_test_curr  = accuracy_score(y_test, y_test_pred)\n",
    "        acc_train_ls.append(max(acc_train_curr, 1 - acc_train_curr))\n",
    "        acc_test_ls.append(max(acc_test_curr, 1 - acc_test_curr))\n",
    "\n",
    "        if verbose:\n",
    "            if t == 2:\n",
    "                print('t\\tPrior\\t\\tTrain\\t\\tTest')\n",
    "            print('%d\\t%.2f\\t\\t%.2f\\t\\t%.2f' % (t - 1, (np.sum(y_sample == 1) / y_sample.size), acc_train_curr, acc_test_curr))\n",
    "            if alpha_t < 0:\n",
    "                print('Alpha %.2f, terminated' % alpha_t)\n",
    "\n",
    "    # To skip initialized 0 in alpha list\n",
    "    alpha = alpha[1:]\n",
    "\n",
    "    # Get final accuracy on best boosting iteration on train set\n",
    "    # Do not record best iteration on test set -- would train hyperparameter on test\n",
    "    max_idx = np.argmax(acc_train_ls)\n",
    "    acc_train_final = acc_train_ls[max_idx]\n",
    "    acc_test_final  = acc_test_ls[max_idx]\n",
    "\n",
    "    if verbose:\n",
    "        print('t = %d was best iteration with accuracy %.2f\\n' % (max_idx + 1, acc_test_final))\n",
    "\n",
    "    # Return minimum error (1 - max accuracy)\n",
    "    return 1 - acc_train_final, 1 - acc_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prior_svm_lambdaboost(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    W,\n",
    "    prior,\n",
    "    T=20,\n",
    "    sample_prop=1,\n",
    "    random_seed=None,\n",
    "    verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.array([1, 2, 3])\n",
    "q_new = q[[1, 2]]\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_lambdaboost(X_train, y_train, X_test, y_test, W, T=20, sample_prop=1, random_seed=None, verbose=True):\n",
    "    \"\"\" Generates linear svm lambdaboost classifier \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('LINEAR SVM SUBMODELS')\n",
    "        print('Using %d classifiers and sample proportion of %d'\n",
    "              % (T, sample_prop))\n",
    "        if random_seed:\n",
    "            print('Random seed %d', random_seed)\n",
    "\n",
    "    # Constants\n",
    "    m = X_train.shape[0]\n",
    "    n = X_train.shape[1]\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Initialize model parameters\n",
    "    f_intercept = 0\n",
    "    f_coefficient = np.zeros(n)\n",
    "\n",
    "    # Initialize counters\n",
    "    t = 1\n",
    "    alpha = [0.0]\n",
    "    acc_train_ls = []\n",
    "    acc_test_ls  = []\n",
    "\n",
    "    # Training\n",
    "    while t <= T and alpha[-1] >= 0:\n",
    "\n",
    "        # Step 6: compute epsilon\n",
    "        curr_pred = np.dot(X_train, f_coefficient) + f_intercept\n",
    "        # Scale predictions, works well empirically\n",
    "        curr_pred = minmax_scale(curr_pred, feature_range=(-1, 1))\n",
    "        # Remember that W is passed as a list of arrays!\n",
    "        epsilon = cal_uncertainty(curr_pred, [W])\n",
    "\n",
    "        # Step 7: compute weights\n",
    "        weight = cal_weights(epsilon)\n",
    "\n",
    "        # Step 8: extract labels\n",
    "        y = np.sign(weight)\n",
    "\n",
    "        # Step 9: create training (sample) data by sampling based on weights\n",
    "        p_weight = np.abs(weight)\n",
    "        p_weight /= np.sum(p_weight)\n",
    "        sample = np.random.choice(m, size=m*sample_prop, replace=True, p=p_weight)\n",
    "        X_sample = X_train[sample]\n",
    "        y_sample = y[sample]\n",
    "\n",
    "        # Step 10: learn binary classifier on training (sample) data\n",
    "        clf = LinearSVC(max_iter=1000)\n",
    "        clf.fit(X_sample, y_sample)\n",
    "\n",
    "        # Step 11: predict labels using current classifier\n",
    "        y_pred = clf.predict(X_train)\n",
    "\n",
    "        # Step 12: compute weight of current classifier\n",
    "        alpha_t = cal_alpha(y_pred, epsilon)\n",
    "\n",
    "        # Make sure alpha is valid\n",
    "        if np.isnan(alpha_t) or np.isinf(alpha_t):\n",
    "            print('Alpha invalid, terminated')\n",
    "            break\n",
    "\n",
    "        # Step 13: update final classifier\n",
    "        f_coefficient += alpha_t * clf.coef_.ravel()\n",
    "        f_intercept += alpha_t * clf.intercept_\n",
    "\n",
    "        # Update loop\n",
    "        alpha.append(alpha_t)\n",
    "        t += 1\n",
    "\n",
    "        # Evaluation\n",
    "        y_train_pred = np.dot(X_train, f_coefficient) + f_intercept\n",
    "        y_test_pred = np.dot(X_test, f_coefficient) + f_intercept\n",
    "        y_train_pred = np.sign(y_train_pred)\n",
    "        y_test_pred = np.sign(y_test_pred)\n",
    "\n",
    "        acc_train_curr = accuracy_score(y_train, y_train_pred)\n",
    "        acc_test_curr  = accuracy_score(y_test, y_test_pred)\n",
    "        acc_train_ls.append(max(acc_train_curr, 1 - acc_train_curr))\n",
    "        acc_test_ls.append(max(acc_test_curr, 1 - acc_test_curr))\n",
    "\n",
    "        if verbose:\n",
    "            if t == 2:\n",
    "                print('t\\tTrain\\t\\tTest\\t\\tPrior')\n",
    "            print('%d\\t%.2f\\t\\t%.2f\\t\\t%.2f' % (t - 1, acc_train_curr, acc_test_curr, (np.sum(y_sample == 1) / y_sample.size)))\n",
    "            if alpha_t < 0:\n",
    "                print('Alpha %.2f, terminated' % alpha_t)\n",
    "\n",
    "    # To skip initialized 0 in alpha list\n",
    "    alpha = alpha[1:]\n",
    "\n",
    "    \"\"\" CHANGED \"\"\"\n",
    "    # # Return final classifier parameters, weight of each submodel\n",
    "    # return f_coefficient, f_intercept, alpha\n",
    "\n",
    "    \"\"\" CHANGED (again) \"\"\"\n",
    "    # acc_train = accuracy_score(y_train, y_train_pred)\n",
    "    # acc_test  =  accuracy_score(y_test, y_test_pred)\n",
    "    # return min(acc_train, 1 - acc_train), min(acc_test, 1 - acc_test)\n",
    "\n",
    "    # Get final accuracy on best boosting iteration on train set\n",
    "    # Do not record best iteration on test set -- would train hyperparameter on test\n",
    "    max_idx = np.argmax(acc_train_ls)\n",
    "    acc_train_final = acc_train_ls[max_idx]\n",
    "    acc_test_final  = acc_test_ls[max_idx]\n",
    "\n",
    "    if verbose:\n",
    "        print('t = %d was best iteration with accuracy %.2f\\n' % (max_idx + 1, acc_test_final))\n",
    "\n",
    "    # Return minimum error (1 - max accuracy)\n",
    "    return 1 - acc_train_final, 1 - acc_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_lambdaboost(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE SUBMODELS\n",
      "Using 20 classifiers and sample proportion of 1\n",
      "t\tTrain\t\tTest\n",
      "1\t0.71\t\t0.72\n",
      "2\t0.31\t\t0.30\n",
      "3\t0.70\t\t0.70\n",
      "4\t0.30\t\t0.28\n",
      "5\t0.71\t\t0.71\n",
      "6\t0.71\t\t0.71\n",
      "7\t0.77\t\t0.80\n",
      "8\t0.77\t\t0.76\n",
      "9\t0.76\t\t0.76\n",
      "10\t0.78\t\t0.76\n",
      "11\t0.78\t\t0.77\n",
      "12\t0.79\t\t0.78\n",
      "13\t0.79\t\t0.79\n",
      "14\t0.79\t\t0.80\n",
      "15\t0.80\t\t0.79\n",
      "16\t0.80\t\t0.79\n",
      "17\t0.80\t\t0.80\n",
      "18\t0.80\t\t0.79\n",
      "19\t0.81\t\t0.80\n",
      "20\t0.81\t\t0.80\n",
      "t = 20 was best iteration with accuracy 0.80\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.18999999999999995, 0.19599999999999995)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_lambdaboost(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prior_tree_lambdaboost(X_train, y_train, X_test, y_test, W, prior, T=20, max_depth=5, sample_prop=1, \n",
    "                                 random_seed=None, verbose=True):\n",
    "    \"\"\" Generates decision tree lambdaboost classifier \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('WITH CLASS PRIOR %.2f' % prior)\n",
    "        print('DECISION TREE SUBMODELS')\n",
    "        print('Using %d classifiers and sample proportion of %d' % (T, sample_prop))\n",
    "        if random_seed:\n",
    "            print('Random seed %d', random_seed)\n",
    "\n",
    "    # Constants\n",
    "    m = X_train.shape[0]\n",
    "    n = X_train.shape[1]\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Initialize counters\n",
    "    t = 1\n",
    "    alpha_t = 0\n",
    "    acc_train_ls = []\n",
    "    acc_test_ls  = []\n",
    "\n",
    "    # Instantiate models and weights\n",
    "    f = []\n",
    "    alpha = []\n",
    "\n",
    "    # Training\n",
    "    while t <= T and alpha_t >= 0:\n",
    "\n",
    "        # Step 6: compute epsilon\n",
    "        if t == 1:\n",
    "            curr_pred = np.zeros(y_train.shape)\n",
    "        else:\n",
    "            curr_pred = sum([alpha[i] * f[i].predict(X_train) for i in range(t - 1)])\n",
    "        # Scale predictions, works well empirically for SVM...\n",
    "        #curr_pred = minmax_scale(curr_pred, feature_range=(-1, 1))\n",
    "        # Remember that W is passed as a list of arrays!\n",
    "        epsilon = cal_uncertainty(curr_pred, [W])\n",
    "\n",
    "        # Step 7: compute weights\n",
    "        weight = cal_weights(epsilon)\n",
    "\n",
    "        # Step 8: extract labels\n",
    "        weight_idx = np.argsort(weight)\n",
    "        X_new = X_train[weight_idx]\n",
    "        y_new = np.concatenate((-np.ones(m - int(m * prior)), np.ones(int(m * prior))))\n",
    "\n",
    "        # Step 9: create training (sample) data by sampling based on weights\n",
    "        # Sorting weights: necessary for sampling by prob in np.random.choice\n",
    "        weight = np.sort(weight)\n",
    "        p_weight = np.abs(weight)\n",
    "        p_weight /= np.sum(p_weight)\n",
    "        sample = np.random.choice(m, size=m*sample_prop, replace=True, p=p_weight)\n",
    "        X_sample = X_new[sample]\n",
    "        y_sample = y_new[sample]\n",
    "\n",
    "        # Step 10: learn binary classifier on training (sample) data\n",
    "        clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "        clf.fit(X_sample, y_sample)\n",
    "\n",
    "        # Step 11: predict labels using current classifier\n",
    "        y_pred = clf.predict(X_train)\n",
    "\n",
    "        # Step 12: compute weight of current classifier\n",
    "        alpha_t = cal_alpha(y_pred, epsilon)\n",
    "\n",
    "        # Make sure alpha is valid\n",
    "        if np.isnan(alpha_t) or np.isinf(alpha_t):\n",
    "            print('Alpha invalid, terminated')\n",
    "            break\n",
    "\n",
    "        # Step 13: update final classifier\n",
    "        f.append(clf)\n",
    "\n",
    "        # Update loop\n",
    "        alpha.append(alpha_t)\n",
    "        t += 1\n",
    "\n",
    "        # Evaluation\n",
    "        y_train_pred = sum([alpha[i] * f[i].predict(X_train) for i in range(t - 1)])\n",
    "        y_test_pred = sum([alpha[i] * f[i].predict(X_test) for i in range(t - 1)])\n",
    "        y_train_pred = np.sign(y_train_pred)\n",
    "        y_test_pred = np.sign(y_test_pred)\n",
    "\n",
    "        acc_train_curr = accuracy_score(y_train, y_train_pred)\n",
    "        acc_test_curr  = accuracy_score(y_test, y_test_pred)\n",
    "        acc_train_ls.append(max(acc_train_curr, 1 - acc_train_curr))\n",
    "        acc_test_ls.append(max(acc_test_curr, 1 - acc_test_curr))\n",
    "\n",
    "        if verbose:\n",
    "            if t == 2:\n",
    "                print('t\\tPrior\\t\\tTrain\\t\\tTest')\n",
    "            print('%d\\t%.2f\\t\\t%.2f\\t\\t%.2f' % (t - 1, (np.sum(y_sample == 1) / y_sample.size), \n",
    "                                                acc_train_curr, acc_test_curr))\n",
    "            if alpha_t < 0:\n",
    "                print('Alpha %.2f, terminated' % alpha_t)\n",
    "\n",
    "    # Get final accuracy on best boosting iteration on train set\n",
    "    # Do not record best iteration on test set -- would train hyperparameter on test\n",
    "    max_idx = np.argmax(acc_train_ls)\n",
    "    acc_train_final = acc_train_ls[max_idx]\n",
    "    acc_test_final  = acc_test_ls[max_idx]\n",
    "\n",
    "    if verbose:\n",
    "        print('t = %d was best iteration with accuracy %.2f\\n' % (max_idx + 1, acc_test_final))\n",
    "\n",
    "    # Return minimum error (1 - max accuracy)\n",
    "    return 1 - acc_train_final, 1 - acc_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH CLASS PRIOR 0.40\n",
      "DECISION TREE SUBMODELS\n",
      "Using 20 classifiers and sample proportion of 1\n",
      "t\tPrior\t\tTrain\t\tTest\n",
      "1\t0.49\t\t0.78\t\t0.81\n",
      "2\t0.42\t\t0.30\t\t0.23\n",
      "3\t0.48\t\t0.79\t\t0.83\n",
      "4\t0.46\t\t0.79\t\t0.83\n",
      "5\t0.52\t\t0.76\t\t0.78\n",
      "6\t0.55\t\t0.78\t\t0.80\n",
      "7\t0.52\t\t0.77\t\t0.79\n",
      "8\t0.41\t\t0.77\t\t0.80\n",
      "9\t0.56\t\t0.78\t\t0.80\n",
      "10\t0.47\t\t0.77\t\t0.81\n",
      "11\t0.47\t\t0.77\t\t0.82\n",
      "12\t0.46\t\t0.78\t\t0.82\n",
      "13\t0.47\t\t0.78\t\t0.81\n",
      "14\t0.53\t\t0.79\t\t0.81\n",
      "15\t0.42\t\t0.79\t\t0.82\n",
      "16\t0.50\t\t0.79\t\t0.82\n",
      "17\t0.48\t\t0.79\t\t0.82\n",
      "18\t0.52\t\t0.80\t\t0.81\n",
      "19\t0.53\t\t0.80\t\t0.81\n",
      "20\t0.47\t\t0.81\t\t0.81\n",
      "t = 20 was best iteration with accuracy 0.81\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.19499999999999995, 0.18799999999999994)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_prior_tree_lambdaboost(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    W,\n",
    "    prior=0.4,\n",
    "    max_depth=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
