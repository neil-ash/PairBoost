{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys; sys.path.insert(0,'..')\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_svmlight_file('../data/w8a/w8a.txt')\n",
    "X_train = X_train.toarray()\n",
    "\n",
    "X_test, y_test = load_svmlight_file('../data/w8a/w8a.t')\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRELIMINARY\n",
      "Training accuracy of supervised SVM: 1.00\n",
      "\n",
      "GENERATE W\n",
      "[[-1. -1.]\n",
      " [-1.  1.]\n",
      " [ 1. -1.]] [184   8   8]\n",
      "Training accuracy of pairwise rank SVM: 0.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)\n",
    "\n",
    "X_train, y_train, X_test, y_test = X_train[:50], y_train[:50], X_test[:500], y_test[:500]\n",
    "\n",
    "rank = generate_preliminary_pairs(X_train, y_train, kernel='linear', random_seed=1)\n",
    "W = generate_from_preliminary_W(X_train, y_train, rank, m=200, kernel='linear', random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEIL'S NEW MODEL\n",
      "LINEAR SVM SUBMODELS\n",
      "Using 20 classifiers and sample proportion of 1\n",
      "t\tTrain\t\tTest\t\tPrior\n",
      "1\t0.98\t\t0.98\t\t0.08\n",
      "2\t0.98\t\t0.98\t\t0.12\n",
      "3\t0.98\t\t0.98\t\t0.08\n",
      "4\t0.96\t\t0.97\t\t0.02\n",
      "5\t0.96\t\t0.97\t\t0.12\n",
      "6\t0.96\t\t0.97\t\t0.06\n",
      "7\t0.96\t\t0.97\t\t0.20\n",
      "8\t0.96\t\t0.97\t\t0.12\n",
      "9\t0.96\t\t0.97\t\t0.10\n",
      "10\t0.96\t\t0.97\t\t0.04\n",
      "11\t0.96\t\t0.97\t\t0.04\n",
      "12\t0.96\t\t0.97\t\t0.12\n",
      "13\t0.96\t\t0.97\t\t0.06\n",
      "14\t0.96\t\t0.97\t\t0.14\n",
      "15\t0.96\t\t0.97\t\t0.04\n",
      "16\t0.96\t\t0.97\t\t0.06\n",
      "17\t0.96\t\t0.97\t\t0.08\n",
      "18\t0.96\t\t0.97\t\t0.06\n",
      "19\t0.96\t\t0.97\t\t0.06\n",
      "20\t0.96\t\t0.97\t\t0.02\n",
      "t = 1 was best iteration with accuracy 0.98\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020000000000000018, 0.02400000000000002)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def new_svm_lambdaboost(X_train, y_train, X_test, y_test, W, prior, T=20, sample_prop=1, random_seed=None, verbose=True):\n",
    "# \"\"\" Generates linear svm lambdaboost classifier \"\"\"\n",
    "prior = 0.04\n",
    "T=20; sample_prop=1; random_seed=None; verbose=True\n",
    "\n",
    "\n",
    "print(\"NEIL'S NEW MODEL\")\n",
    "\n",
    "if verbose:\n",
    "    print('LINEAR SVM SUBMODELS')\n",
    "    print('Using %d classifiers and sample proportion of %d'\n",
    "          % (T, sample_prop))\n",
    "    if random_seed:\n",
    "        print('Random seed %d', random_seed)\n",
    "\n",
    "# Constants\n",
    "m = X_train.shape[0]\n",
    "n = X_train.shape[1]\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Initialize model parameters\n",
    "f_intercept = 0\n",
    "f_coefficient = np.zeros(n)\n",
    "\n",
    "# Initialize counters\n",
    "t = 1\n",
    "alpha = [0.0]\n",
    "acc_train_ls = []\n",
    "acc_test_ls  = []\n",
    "\n",
    "# Training\n",
    "while t <= T and alpha[-1] >= 0:\n",
    "\n",
    "    # Step 6: compute epsilon\n",
    "    curr_pred = np.dot(X_train, f_coefficient) + f_intercept\n",
    "    # Scale predictions, works well empirically\n",
    "    curr_pred = minmax_scale(curr_pred, feature_range=(-1, 1))\n",
    "    # Remember that W is passed as a list of arrays!\n",
    "    epsilon = cal_uncertainty(curr_pred, [W])\n",
    "\n",
    "    # Step 7: compute weights\n",
    "    weight = cal_weights(epsilon)\n",
    "\n",
    "    # Step 8: extract labels\n",
    "    #y = np.sign(weight)\n",
    "#     if t == 1:\n",
    "#         weight = np.sort(weight)\n",
    "#         y = np.concatenate((-np.ones(m - int(m * prior)), np.ones(int(m * prior))))\n",
    "#     else:\n",
    "#         y = np.sign(weight)\n",
    "    weight_idx = np.argsort(weight)\n",
    "    X_new = X_train[weight_idx]\n",
    "    y_new = np.concatenate((-np.ones(m - int(m * prior)), np.ones(int(m * prior))))\n",
    "\n",
    "    # Step 9: create training (sample) data by sampling based on weights\n",
    "    weight = np.sort(weight) # Added, necessary for sampling probability\n",
    "    p_weight = np.abs(weight)\n",
    "    p_weight /= np.sum(p_weight)\n",
    "    sample = np.random.choice(m, size=m*sample_prop, replace=True, p=p_weight)\n",
    "    X_sample = X_new[sample]\n",
    "    y_sample = y_new[sample]\n",
    "    \n",
    "#     \"\"\" REMOVE \"\"\"\n",
    "#     X_sample = X_new\n",
    "#     y_sample = y_new\n",
    "\n",
    "    # Step 10: learn binary classifier on training (sample) data\n",
    "    clf = LinearSVC(max_iter=1000)\n",
    "    clf.fit(X_sample, y_sample)\n",
    "\n",
    "    # Step 11: predict labels using current classifier\n",
    "    y_pred = clf.predict(X_train)\n",
    "\n",
    "    # Step 12: compute weight of current classifier\n",
    "    alpha_t = cal_alpha(y_pred, epsilon)\n",
    "\n",
    "    # Make sure alpha is valid\n",
    "    if np.isnan(alpha_t) or np.isinf(alpha_t):\n",
    "        print('Alpha invalid, terminated')\n",
    "        break\n",
    "\n",
    "    # Step 13: update final classifier\n",
    "    f_coefficient += alpha_t * clf.coef_.ravel()\n",
    "    f_intercept += alpha_t * clf.intercept_\n",
    "\n",
    "    # Update loop\n",
    "    alpha.append(alpha_t)\n",
    "    t += 1\n",
    "\n",
    "    # Evaluation\n",
    "    y_train_pred = np.dot(X_train, f_coefficient) + f_intercept\n",
    "    y_test_pred = np.dot(X_test, f_coefficient) + f_intercept\n",
    "    y_train_pred = np.sign(y_train_pred)\n",
    "    y_test_pred = np.sign(y_test_pred)\n",
    "\n",
    "    acc_train_curr = accuracy_score(y_train, y_train_pred)\n",
    "    acc_test_curr  = accuracy_score(y_test, y_test_pred)\n",
    "    acc_train_ls.append(max(acc_train_curr, 1 - acc_train_curr))\n",
    "    acc_test_ls.append(max(acc_test_curr, 1 - acc_test_curr))\n",
    "\n",
    "    if verbose:\n",
    "        if t == 2:\n",
    "            print('t\\tTrain\\t\\tTest\\t\\tPrior')\n",
    "        print('%d\\t%.2f\\t\\t%.2f\\t\\t%.2f' % (t - 1, acc_train_curr, acc_test_curr, (np.sum(y_sample == 1) / y_sample.size)))\n",
    "        if alpha_t < 0:\n",
    "            print('Alpha %.2f, terminated' % alpha_t)\n",
    "            \n",
    "# To skip initialized 0 in alpha list\n",
    "alpha = alpha[1:]\n",
    "\n",
    "# Get final accuracy on best boosting iteration on train set\n",
    "# Do not record best iteration on test set -- would train hyperparameter on test\n",
    "max_idx = np.argmax(acc_train_ls)\n",
    "acc_train_final = acc_train_ls[max_idx]\n",
    "acc_test_final  = acc_test_ls[max_idx]\n",
    "\n",
    "if verbose:\n",
    "    print('t = %d was best iteration with accuracy %.2f\\n' % (max_idx + 1, acc_test_final))\n",
    "\n",
    "# Return minimum error (1 - max accuracy)\n",
    "1 - acc_train_final, 1 - acc_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq1 = np.sum(y_test == 1) / y_test.size\n",
    "eq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_lambdaboost(X_train, y_train, X_test, y_test, W, T=20, sample_prop=1, random_seed=None, verbose=True):\n",
    "    \"\"\" Generates linear svm lambdaboost classifier \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('LINEAR SVM SUBMODELS')\n",
    "        print('Using %d classifiers and sample proportion of %d'\n",
    "              % (T, sample_prop))\n",
    "        if random_seed:\n",
    "            print('Random seed %d', random_seed)\n",
    "\n",
    "    # Constants\n",
    "    m = X_train.shape[0]\n",
    "    n = X_train.shape[1]\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Initialize model parameters\n",
    "    f_intercept = 0\n",
    "    f_coefficient = np.zeros(n)\n",
    "\n",
    "    # Initialize counters\n",
    "    t = 1\n",
    "    alpha = [0.0]\n",
    "    acc_train_ls = []\n",
    "    acc_test_ls  = []\n",
    "\n",
    "    # Training\n",
    "    while t <= T and alpha[-1] >= 0:\n",
    "\n",
    "        # Step 6: compute epsilon\n",
    "        curr_pred = np.dot(X_train, f_coefficient) + f_intercept\n",
    "        # Scale predictions, works well empirically\n",
    "        curr_pred = minmax_scale(curr_pred, feature_range=(-1, 1))\n",
    "        # Remember that W is passed as a list of arrays!\n",
    "        epsilon = cal_uncertainty(curr_pred, [W])\n",
    "\n",
    "        # Step 7: compute weights\n",
    "        weight = cal_weights(epsilon)\n",
    "\n",
    "        # Step 8: extract labels\n",
    "        y = np.sign(weight)\n",
    "\n",
    "        # Step 9: create training (sample) data by sampling based on weights\n",
    "        p_weight = np.abs(weight)\n",
    "        p_weight /= np.sum(p_weight)\n",
    "        sample = np.random.choice(m, size=m*sample_prop, replace=True, p=p_weight)\n",
    "        X_sample = X_train[sample]\n",
    "        y_sample = y[sample]\n",
    "\n",
    "        # Step 10: learn binary classifier on training (sample) data\n",
    "        clf = LinearSVC(max_iter=1000)\n",
    "        clf.fit(X_sample, y_sample)\n",
    "\n",
    "        # Step 11: predict labels using current classifier\n",
    "        y_pred = clf.predict(X_train)\n",
    "\n",
    "        # Step 12: compute weight of current classifier\n",
    "        alpha_t = cal_alpha(y_pred, epsilon)\n",
    "\n",
    "        # Make sure alpha is valid\n",
    "        if np.isnan(alpha_t) or np.isinf(alpha_t):\n",
    "            print('Alpha invalid, terminated')\n",
    "            break\n",
    "\n",
    "        # Step 13: update final classifier\n",
    "        f_coefficient += alpha_t * clf.coef_.ravel()\n",
    "        f_intercept += alpha_t * clf.intercept_\n",
    "\n",
    "        # Update loop\n",
    "        alpha.append(alpha_t)\n",
    "        t += 1\n",
    "\n",
    "        # Evaluation\n",
    "        y_train_pred = np.dot(X_train, f_coefficient) + f_intercept\n",
    "        y_test_pred = np.dot(X_test, f_coefficient) + f_intercept\n",
    "        y_train_pred = np.sign(y_train_pred)\n",
    "        y_test_pred = np.sign(y_test_pred)\n",
    "\n",
    "        acc_train_curr = accuracy_score(y_train, y_train_pred)\n",
    "        acc_test_curr  = accuracy_score(y_test, y_test_pred)\n",
    "        acc_train_ls.append(max(acc_train_curr, 1 - acc_train_curr))\n",
    "        acc_test_ls.append(max(acc_test_curr, 1 - acc_test_curr))\n",
    "\n",
    "        if verbose:\n",
    "            if t == 2:\n",
    "                print('t\\tTrain\\t\\tTest\\t\\tPrior')\n",
    "            print('%d\\t%.2f\\t\\t%.2f\\t\\t%.2f' % (t - 1, acc_train_curr, acc_test_curr, (np.sum(y == 1) / y.size)))\n",
    "            if alpha_t < 0:\n",
    "                print('Alpha %.2f, terminated' % alpha_t)\n",
    "\n",
    "    # To skip initialized 0 in alpha list\n",
    "    alpha = alpha[1:]\n",
    "\n",
    "    \"\"\" CHANGED \"\"\"\n",
    "    # # Return final classifier parameters, weight of each submodel\n",
    "    # return f_coefficient, f_intercept, alpha\n",
    "\n",
    "    \"\"\" CHANGED (again) \"\"\"\n",
    "    # acc_train = accuracy_score(y_train, y_train_pred)\n",
    "    # acc_test  =  accuracy_score(y_test, y_test_pred)\n",
    "    # return min(acc_train, 1 - acc_train), min(acc_test, 1 - acc_test)\n",
    "\n",
    "    # Get final accuracy on best boosting iteration on train set\n",
    "    # Do not record best iteration on test set -- would train hyperparameter on test\n",
    "    max_idx = np.argmax(acc_train_ls)\n",
    "    acc_train_final = acc_train_ls[max_idx]\n",
    "    acc_test_final  = acc_test_ls[max_idx]\n",
    "\n",
    "    if verbose:\n",
    "        print('t = %d was best iteration with accuracy %.2f\\n' % (max_idx + 1, acc_test_final))\n",
    "\n",
    "    # Return minimum error (1 - max accuracy)\n",
    "    return 1 - acc_train_final, 1 - acc_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_lambdaboost(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
